## table & index size

--table / index size
SELECT pg_size_pretty(pg_table_size('sample'));
 
--list indexes on table
select * from pg_indexes where tablename = 'sample';
 
--list indexes and size on table
select a.relname, pg_size_pretty(pg_relation_size(a.oid)), b.indexdef
from pg_class a
left join pg_indexes b on a.relname = b.indexname
where b.tablename = 'sample';
 
--disable sequential scan
set enable_seqscan = off;
 
--disable index scan
set enable_indexscan = off;


## select without table  
* [Select_without_tables](https://en.wikibooks.org/wiki/SQL_Dialects_Reference/Select_queries/Select_without_tables)

select * from (values (1, 'one', '一'), (2, 'two', '二'), (3, 'three', '三')) as t (num,letter,chinese);
 
select unnest(array['foo', 'bar', 'fooBar']);


## last value of sequence

SELECT last_value FROM your_sequence_name;


## pg_restore with 5 jobs

time nohup pg_restore -U groot -d groot -v -j 5 file_name


## vacuumdb with 5 jobs

#執行 DB 重整 設定 5 個 job
time nohup vacuumdb -h 127.0.0.1 -U postgres -d groot -v -z -j 5


## psql memory/workers parameters

show work_mem;
show maintenance_work_mem;
show max_parallel_workers;
show max_parallel_maintenance_workers;
 
set work_mem = '512MB';


## 檢查是否為數字

select textregexeq('12345a','^[[:digit:]]+(\.[[:digit:]]+)?$')
 
--檢查是否含有數字 括弧 大小於符號
select textregexeq(trim(value), '[\d\.\(\)><＜＞]')
 
--檢查只含有數字
select textregexeq('O422520989', '^[0-9]+$' )


## 有人連線, 無法刪除 database 的解法

psql 以 postgres 登入
 
REVOKE CONNECT ON DATABASE groot FROM public;
 
\c groot
 
--terminate connections, probably need repeat multiple times
SELECT pid, pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE datname = current_database() AND pid <> pg_backend_pid();
 
--跳出 database groot
\c postgres
 
--drop database
drop database groot;
 
------------
--如果有持續重連的 client, 無法 drop database 時
--https://dba.stackexchange.com/questions/11893/force-drop-db-while-others-may-be-connected
psql 以 postgres 登入
 
UPDATE pg_database SET datallowconn = 'false' WHERE datname = 'groot';
or
ALTER DATABASE groot CONNECTION LIMIT 0; (Superusers still can connect!)
 
SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'groot';
 
drop database groot;


## Reload configs, not interrupt queries and connections

su - postgres
/usr/bin/pg_ctl reload
 
# or using SQL
SELECT pg_reload_conf();


## 目前工作階段

--show track_activities;
select * from pg_stat_activity
where usename = 'groot'
and state <> 'idle'
 
select pg_terminate_backend(xxxxx)

## PostgreSQL distinct 特殊語法

--https://iter01.com/537180.html
--DISTINCT ON (column): 與 rank(), row_number() 選第1筆的效果相同
select distinct on (id) id, date, another_info
from the_table
order by id, date desc;
 
--IS [NOT] DISTINCT FROM: 可比較 null 值
WITH t AS (
  SELECT 1 AS a, 1 AS b
  UNION ALL
  SELECT 1, 2
  UNION ALL
  SELECT NULL, 1
  UNION ALL
  SELECT NULL, NULL )
SELECT a, b, a IS NOT DISTINCT FROM b "a=b"
FROM t;
 
--aggregate function 內也可以用 distinct, count(), sum(), string_agg()...


## Python function 需要 superuser 才能建立的解法

--python function 需要 superuser 權限才能建立
--可建立臨時的 superuser role: dba, 賦予 user groot
create role dba with superuser noinherit;
grant dba to groot;
 
--user groot 可以切換到 role dba, 建立 function 後再換回一般權限
set role dba;
reset role;


## 清除不再使用的 lob 檔

https://www.postgresql.org/docs/12/vacuumlo.html
--remove orphaned large objects
vacuumlo -U groot -v groot
 
--ex.
drop table news;
 
vacuumlo -U groot -v groot
 
--pg_restore dump file
pg_restore -U groot -d groot -v groot_news_demo_20211020.tar


## pgAdmin 4 預設只回傳 1000 筆資料的解法

Changing the parameter in config.py pgAdmin4/web folder
 
ON_DEMAND_RECORD_COUNT = 100000000


## 列出目前的 Lock 

SELECT
  COALESCE(blockingl.relation::regclass::text,blockingl.locktype) as locked_item,
  now() - blockeda.query_start AS waiting_duration, blockeda.pid AS blocked_pid,
  blockeda.query as blocked_query, blockedl.mode as blocked_mode,
  blockinga.pid AS blocking_pid, blockinga.query as blocking_query,
  blockingl.mode as blocking_mode
FROM
    pg_catalog.pg_locks blockedl
JOIN
    pg_stat_activity blockeda
ON
    blockedl.pid = blockeda.pid
JOIN
    pg_catalog.pg_locks blockingl
ON
    (((blockingl.transactionid=blockedl.transactionid) OR
    (blockingl.relation=blockedl.relation AND blockingl.locktype=blockedl.locktype)) AND
     blockedl.pid != blockingl.pid)
JOIN pg_stat_activity blockinga ON blockingl.pid = blockinga.pid
  AND blockinga.datid = blockeda.datid
WHERE NOT blockedl.granted


## COALESCE

select COALESCE(value_1,value_2,value_3,........value_n)...


## 安裝 PostgreSQL 新版遇到 dependency 問題的解法

# require an extra repo for llvm and clang
# reference: https://yum.postgresql.org/news/devel-rpms-require-a-new-repository/
 
sudo yum -y install centos-release-scl
sudo yum -y install epel-release
sudo yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm


https://www.postgresql.org/docs/12/monitoring-stats.html

--statistics of index
select * from pg_stat_user_indexes where indexrelname in ('idx_subject_gid');


## Convert excel to csv

# Batch convert excel to csv by libreOffice
# 44: comma, 34: double quote, 76: UTF-8
libreoffice --headless --convert-to csv:"Text - txt - csv (StarCalc)":44,34,76,,,,true --outdir converted *.*
 
# convert by csvkit
# Numbers that look like integers from an XLS will have decimals in CSV, but those from an XLSX won't.
in2csv 檢驗報告單位_20221101_REG\ \(1\).xls > 檢驗報告單位_20221101_REG\ \(1\).csv
OpenOffice Token_8.2C_csv_export



## Batch remove digits in file name

ls *.csv | sed -e "s/[0-9]//g"
 
# 去除檔名的數字
for file in *; do
    [[ -f "${file}" ]] && [[ "${file}" =~ (^[0-9]+) ]] && mv ${file} ${file/${BASH_REMATCH[1]}}
done
 
# 去除檔名的"100"
for file in *; do
    [[ -f "${file}" ]] && [[ "${file}" =~ (100) ]] && mv ${file} ${file/${BASH_REMATCH[1]}}
done


## Import table dynamically

[dynamic_copy_csv.sql] attach file 



## Vacuum analyze all table in schema
===
## vacuum analyze all tables in schema 'cgmh_lk'
psql -t -A -U groot -c "select format('vacuum verbose analyse %I.%I;', n.table_schema::varchar, n.table_name::varchar) from information_schema.tables n where table_schema = 'cgmh_lk' order by 1;" | psql -U groot
===

## Dump & restore schema
===
# dump schema(without blob)
pg_dump -U groot -d groot -Fd -v -n cgmh_lk -j 3 -f groot_cgmh_lk
 
pg_dump -U groot -d groot -Fd -v -n omop_vghtc -j 3 -f groot_omop_vghtc
 
# restore schema
pg_restore -U groot -d groot -v -j 3 groot_cgmh_lk
 
pg_restore -U groot -d groot -v -j 3 groot_omop_vghtc
===

## csvkit

--list column name
csvcut -n  measurement_202303131129.csv
 
--generate create table script
head -n 10 measurement_202303131129.csv > measurement_202303131129_10.csv
csvsql -i postgresql measurement_202303131129_10.csv
 
--fix by csvclean
csvclean -v r10003_rcgrdocrordf.csv
533 errors logged to r10003_rcgrdocrordf_err.csv
1066 rows were joined/reduced to 533 rows after eliminating expected internal line breaks.


## search_path

SHOW search_path;
SET search_path TO public, pgunit;
 
ALTER DATABASE groot SET search_path TO public, pgunit;
ALTER DATABASE groot SET search_path = "$user", public, pgunit;


## Set parallel 

show parallel_setup_cost;
set parallel_setup_cost = default;
 
show parallel_tuple_cost;
set parallel_tuple_cost = default;
 
show force_parallel_mode;
set force_parallel_mode = default;


## Run PL/pgSQL code snippet

do
$$
declare
    mapping record;
     
begin
    for mapping in (...
        )
    loop
        ...
    end loop;
 
 
    exception
        when others then           
            raise notice '% %', SQLERRM, SQLSTATE;
end
$$


## Add PK serial

--add id serial primary key
ALTER TABLE xxx ADD COLUMN id bigint GENERATED ALWAYS AS IDENTITY PRIMARY KEY;


## psql script multi-line

echo 'select count(*)
from
groot_user;' | psql -U groot -d groot


## 可試著用 tar 來修復 PostgreSQL 檔案錯誤

-- https://stackoverflow.com/questions/57675636/why-i-have-this-problem-could-not-read-block-in-file-problem
 
--could not read block 8116483 in file "pg_tblspc/26776613/PG_12_201909212/170999254/171001222.61": read only 4096 of 8192 bytes XX001
 
--tar corrupt file
tar -zcvf pg_tbspc_backup.tar.gz /media/data/table_space/groot/PG_12_201909212/170999254/171001222.61
tar: 從成員名稱中移除前端的「/」
/media/data/table_space/groot/PG_12_201909212/170999254/171001222.61
tar: /media/data/table_space/groot/PG_12_201909212/170999254/171001222.61：檔案縮小了 81760256 位元組；以零值填補
 
--extract & replace file, then stop & start PostgreSQL
tar -zxvf pg_tbspc_backup.tar.gz -C ./test


## pgsql_tmp 佔用太多空間

--DB crash 或是其他情形下, table_space/.../pgsql_tmp 目錄下的暫存檔會佔用許多空間, 重啟 database 會自動刪除; 否則要手動刪掉才會釋放出空間


